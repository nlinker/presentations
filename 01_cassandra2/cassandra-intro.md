https://slides.com/nikolaylinker/apache-cassandra/edit

Введение в Кассандру, и вообще NoSQL

Несмотря на дикий распиар где-то лет 5 назад, и последующее охлаждение интереса к NoSql, тем не менее
NoSql хранилища заняли заметную нишу в практике построения программных систем. Давайте разберёмся,
откуда взялась необходимость в подобных решениях.

## Откуда

1. Мир изменился. Информации стало много, и для её обработки нужны большие системы.
    Если раньше большие системы представляли собой мощный сервер, который хостил множество
    маленьких, средних или больших приложений, то сейчас чаще всего ситуация обратная - одно приложение
    хостится на нескольких серверных нодах.
    Той старой модели до сих пор следует Джавовский сервлет апи, который исходит из предположения, что
    программисты сделают много маленьких приложений, и они все будут лежать в большом Томкате или Глассфише,
    между ними будет изоляция через класслоадеры и всё будет хорошо. А на самом деле изоляции через
    класслоадеры оказывается сильно недостаточно, процессор и диск не так и легко расшарить, и в подавляющем
    большинстве случаев Томкат поднимается чтобы захостить одно единственное приложение. С изоляцией и
    распределением ресурсов например виртуализация справляется намного лучше.
    Итак: одно приложение - одна (или больше) нод.

2. Распределённые системы сталкиваются с фундаментальными трудностями при работе в сети, а именно:
    1. Сеть ненадёжна
    1. Сеть неоднородна
    1. Задержки всегда больше нуля
    1. Пропускная способность сети ограничена
    1. Топология меняется
    1. Цена передачи информации ненулевая
    1. Плюс админов больше 1, каждый из них может делать правильно, но в итоге получится фейл
    1. Присутствие уязвимостей (отдельная большая тема)

3. Теоретические изыскания на тему распределённых систем, в частности CAP теорема
    (её уже все знают, но я немного расскажу).
    Товарищи Seth Gilbert и Nancy Lynch в 2003-м году дали вроде бы формальное доказательство,
    однако это доказательство до сих пор критикуется за его неформальность. В том смысле, что
    отсутствует формальная система (как обычно, с алфавитом, аксиомами и правилами вывода),
    а авторы оперируют терминами "связь", "сервис", "сеть" в своей статье. Тем не менее,
    справедливость CAP теоремы не вызывает ни у кого сомнений. Утверждение, CAP теоремы
    обычно рисуют кружочками.
    И утверждается, что выбрать можно только 2 из кружочков. Единственный вырожденный случай,
    который может удовлетворять всем трём требованиям - это система без состояния.
    Важно отметить, что на практике картинка как правило не чёрно-белая, а с полутонами. Например,
    да гарантии целостности нет, но практически она всегда присутствует. А потеря целотности данных
    распознаётся допустим самим приложением, и оно выкидывает красный флаг и например потом ночью инженеры
    запускают какой-нибудь чистильщик, который устраняет противоречивость данных. Или доступность
    (Availability) не гарантируется, но достигается например крутым железом с батарейками.
    То есть CAP теорема говорит о том, что невозможно гарантировать все 3 свойства сразу, но можно
    на практике система может работать хорошо при отсутствии теоретических гарантий.

4. Исходя из вышеперечисленного, масштабирование вообще, и СУБД в частности сталкивается с
   принципиальными и техническими трудностями.

5. NoSQL базы данных - это хранилища данных, призванные решить проблему масштабирования.
   Как правило такие решения опираются на специфику модели данных и жертвуют какими-то
   гарантиями из ACID.

## Масштабирование SQL СУБД

Давайте представим, что у нас есть простой веб-сервис: стейтлесс фронтенд, за которым обычная СУБД.
Проблемы масштабирования связаны с возможным ростом.

1. Рост объёма данных
1. Рост нагрузок

Это может быть связано с тем, что

1. Требования на задержки ужесточились
    a. Количество одинаковых запросов на чтение к этому веб-сервису возросло
    a. Количество одинаковых запросов на запись к этому веб-сервису возросло
1. Количество данных в базе данных возросло
    a. Количество запросов на чтение одного и того же типа (но скажем от разных пользователей) возросло
    a. Количество запросов на запись одного и того же типа (но скажем от разных пользователей) возросло
    a. Количество передаваемых/возвращаемых данных в запросе возросло (был текст, стали картиночки, музычка и видосики)
1. Требования на даунтайм ужесточились
1. Ещё чего-нибудь
(Или мы предполагаем, что так будет, и хотим подготовиться к нему)

Горизонтальное и вертикальное масштабирование.

### Кэширование
Схема позволяет уменьшить задержки во время операций чтения.

Недостатки:

1. Память для кэша небесплатна
2. Имеет смысл использовать для запросов, которые возвращают мало данных
3. Проблема инвалидации кэша
4. Работы с БД => система в дауне
5. SPOF

### 1 мастер n слейвов
Эта схема масштабирования позволяет увеличить количество запросов на чтение.
Фронтенд пишет только в мастер, а читает только со слейвов. Между мастером и
слейвами настроена асинхронная репликация.

Недостатки:

1. Мастер занят постоянным проталкиванием изменений
2. Задержка между обновлением записи и видимым изменением
3. Система хрупка по отношению к ненадёжной связи между мастером и слейвом
4. Работы с БД => система в дауне
5. SPOF

### Шардинг
Позволяет увеличить количество запросов на запись и вырваться из ограничения
памяти на 1 сервер.

Недостатки/ограничения:

1. Никаких JOIN, FK, триггеров и stored proc
2. Read uncommitted
3. Короткие транзакции (< 100ms)
4. Никаких массовых UPDATE и DELETE
5. Index only, любой запрос с последовательным доступом - fail
6. Работы с БД => система в дауне
7. Связанные данные (список друзей)
8. Плохая отказоустойчивость (это если ещё не учитывать сеть!)
   a. 1 нода -> раз в 3 года
   a. 64 нод -> раз в 3 недели
   a. 200 нод -> больше 1 раза в день
9. Необходимо дублирование каждого шарда
10. Неравномерная нагрузка на ноды (Анджелина Джоли)
11. Проблемы с транзакциями и контролем целостности
12. Приложение должно знать всё о разспределении данных

### PostgreSQL Partitioning
Основная идея - мы берём одну огромную таблицу (master table) и пилим на маленькие (child tables).
Master table - это таблица-шаблон, по которому создаются child tables. Это нормальная таблица, но
не содержит данных и требует триггер. Child tables содержат собственно данные.
Partition Function - это хранимка, которая определяет, какая child table должна содержать новую запись.

1. Почти шардинг, но с сохранением преимущества работы со стандартной SQL СУБД
1. Как правило необходима денормализации
1. Ключи для деления никогда не должны меняться
1. Ручное распределение ключей
1. SPOF, даунтайм при обслуживании
1. Обеспечение отказоустойчивости требует большого труда

Вкратце, это решение годится, если у нас есть одна (в крайнем случае две) очень большая таблица,
и есть необъезжаемые аргументы против NoSQL хранилищ.
Можно рассматривать как промежуточный шаг при движении в сторону специализированного NoSQL
хранилища.

## Грубая классификация NoSQL

1. Key-value (memcached, redis, aerospike, berkley, ...)
2. Документо-ориентированные (MongoDB, Couchbase, Amazon SimpleDB, ...)
3. Колоночные (Cassandra, MonetDB, Amazon RedShift, Google BigQuery, Teradata, HP Vertica, ...)
4. Графовые (Neo4j, OrientDB, FlockDB, ...)

А вообще, смотрите картинку :-)

## Apache Cassandra

Тут на сцену врывается Apache Cassandra чтобы спасти наши данные, и мир заодно. Если серьёзно, то
Кассандра призвана решить многие проблемы масштабирования, и действительно их решает.

Фичи, которыми обладает Кассандра:

1. Линейная масштабируемость, пропускную способность можно вычислить по простой формуле
   `op/sec = C*nodes*cores/RF`, где `C` равно примерно
   a. 3000-4000 для не SSD
   b. ~12000 для SSD
1. Только один тип сервера (Cassandra node)
1. нет SPOF
1. Все ноды держат данные и отвечают на запросы
1. Архитектура кольца
1. Хорошая отказоустойчивость
1. Поддержка множества дата-центров из коробки
1. Поддержка языка запросов CQL
1. Работает на джаве

### Hash Ring

RF = Replication factor = количество нод, содержащих копию значения.
1 означает что данные пишутся только в одну ноду в кластере (никаких копий)

Какая нода будет использована? Partitioner, по умолчанию RandomPartitioner
"mykey" -> 0x1235462 -> номер ноды

PlacementStrategy - куда писать реплики.
SimpleStrategy (по умолчанию) - положить запись в ноду по токену, и остальные
реплики по часовой стрелке. Есть и другие, типа NetworkTopologyStrategy, которые
учитывают топологию сети.

### CAP теорема в случае Кассандры

AP, запросы имеют свои собственные уровни целостности.
Eventual Consistency - если операции записи прекратить, то в конце-концов состояние
кластера станет консистентным.

### Модель данных

Записи идентифицируются по row key. Row key уникален, и никогда не меняется.

|column family "users"
|------------------------------------
|user620  | список колонок для user620
|user20   | список колонок для user20

|column family "playlists"
|---------------------------------------
|playlist123 | список колонок для playlist123
|playlist234 | список колонок для playlist234

Имена колонок могут быть составными, например 20150505|user620

-------------------------
Database     | Keyspace
Table        | Column Family
Primary key  | Row key
Column name  | Column name *or* key
Column value | Column value

Правильнее сказать, Кассандра реализует структуру данных
`Map<RowKey, SortedMap<ColumnKey, ColumnValue>>`

Моделирование данных в Кассандре - отдельная тема

### Как Кассандра выполняет операцию записи

1. Операцию можно посылать любой ноде, она станет координатором
1. Первым делом значение запишется в WAL.
1. Затем операция пишется в MemTable
1. MemTable периодически сбрасываются на диск как SSTable (Sorted String Table)
1. SSTable индекс всегда держится в памяти
1. Новые MemTable-ы создаются в памяти
1. Удаления - это специальная операция записи "tombstone"

### Что такое SSTable?

1. Неизменяемый файл
1. С возможными дырками в виде tombstones и старыми значениями
1. Каждая операция несёт с собой timestamp
1. Одна и та же колонка может быть в разных SSTable-ах
1. Фоновая операция слияния, называемая compaction

Пример SSTable

Writes are always done in memory and hence are always fast.
Once the MemTable reaches a certain size, it is flushed to disk as an immutable SSTable.
However, we will maintain all the SSTable indexes in memory, which means that for any read
we can check the MemTable first, and then walk the sequence of SSTable indexes to find our data.
Once the SSTable is on disk, it is immutable, hence updates and deletes can't touch the data.
Instead, a more recent value is simply stored in MemTable in case of update, and a "tombstone"
record is appended for deletes. Because we check the indexes in sequence, future reads will
find the updated or the tombstone record without ever reaching the older values!
Finally, having hundreds of on-disk SSTables is also not a great idea, hence periodically
we will run a process to merge the on-disk SSTables, at which time the update and delete
records will overwrite and remove the older data.

### Как Кассандра выполняет операцию чтения?

1. Операцию можно посылать любой ноде, она станет координатором
1. Проверяется MemTable
1. Читается SSTable Index
1. Выполняется операция над Bloom Filter
1. Если ок, то ищется нужный файл и делается fseek
1. Читается запись

### Bloom Filter

Bloom Filter - если запись есть, то он скажет да. Но если записи нет, то иногда тоже скажет да.
Реализован как последовательность hash-функций

### Что всё это даёт

Итак, запись выполняется всегда в память, и потому быстро, чтение возможно затронет SSTable на диске,
и может случиться небольшая задержка.

Тем не менее тесты показывают, на стандартных серверах кластер из 4х машин
показывает 10000 оп/сек на запись и чтение.

### Consistency Level

Когда операция считается успешной?

1. Зависит от операции
1. Для разных операций мы можем требовать разные критерии успешности
1. Успех связан с тем, сколько нод ответило о завершении операции

Уровни для записи:
------------------
Any - операция успешна, если любая доступная нода ответила ок
One - операция успешна, если любая нода соответствующая ключу (первичная или реплика) ответила ок
Quorum - операция успешна, если большинство нод ответили ок
Local_Quorum - операция успешна, если большинство нод в локальном ДЦ ответили ок
Each_Quorum - операция успешна, если большинство нод в каждом ДЦ ответили ок
All - операция успешна, если все ноды-реплики соответстующие ключу ответили ок

Если какая-то из нод падает, то хинт записывается на доступную реплику. Затем операция будет возобновлена,
как только нода поднимется.

Уровни для чтения:
------------------
One - операция успешна, если прочитали с ближайшей ноды с данными
Quorum - операция успешна, если прочитали с большинства нод с данными
Local_Quorum - то же самое, но для нод в локальном ДЦ
Each_Quorum - то же самое, но для нод во всех ДЦ
All - операция успешна, если прочитали со всех реплик

### Когда Кассандра хороша

1. Нереляционные данные
1. Постоянные данные
1. Хранение данных в том же формате, в котором предполагается использовать

### Когда Кассандра не так хороша

1. Различные ракурсы тех же самых данных (придётся делать в приложении или отдельно)
1. Реализация очередей
1. Сильно меняющиеся индексированные данные

## Ресурсы, ссылки

Thrift
http://wiki.apache.org/cassandra/ThriftExamples
Hector
http://hector-client.github.io/hector/build/html/index.html
Astyanax
https://github.com/Netflix/astyanax/wiki
JDBC
http://code.google.com/a/apache-extras.org/p/cassandra-jdbc/
DATASTAX Java Driver
http://www.datastax.com/documentation/developer/java-driver/1.0/webhelp/index.html

.NET/C#, C++, Clojure, Erlang, Go, Haskell, Java, Node.js, ODBC, Perl, PHP, Python, Ruby, Rust, Scala (+Spark)
http://planetcassandra.org/client-drivers-tools/
